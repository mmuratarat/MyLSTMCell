# MyLSTMCell
A common LSTM unit is composed of 4 different gates with each having its own weight matrices and biases. Even though initial weights are simply initialized usually according to some random distribution, e.g., Xavier Initialization, sometimes, a custom weight initialization might be needed. However, Tensorflow's built-in LSTM function (tf.keras.layers.LSTM) does not allow for custom weight initialization.

This LSTM cell class implementation is inheriting from Tensorflow's LSTMCell class. You can freeze an LSTM layer and initialize next layer with the previously trained weights, or you can use the pre-trained weights in a subsequent model. By doing so, you might have three main advantages: (i) you could build the network from scratch as deep as needed in real time, (ii) speed: adding each new layer amounts to training a shallow network with only one hidden layer; and (iii) resilience to overfitting. The process of freezing old layers and inserting new ones is repeated until additional layers cease to improve performance. This indicates that itâ€™s time to stop adding new layers and consider the network complete. So we avoid the pitfalls of backpropagation, including its high computational cost and its struggle to effectively adjust deep parameters. This also substantially decreases training time.
